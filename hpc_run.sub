#!/bin/bash
#SBATCH --qos medium
#SBATCH --account iacc_gbuzzell
#SBATCH --partition 6g-per-core

# get collection of all available subjects
ls /home/data/NDClab/data/base-eeg/CMI/rawdata/ -F | grep / > subjects.txt
SUBJ_LINES=$(wc -l subjects.txt)
COUNT=${SUBJ_LINES:0:1}

#SBATCH --nodes=1                # use a single compute node
#SBATCH --array=0-${COUNT}         # create as many arrays as subjects
#SBATCH --time=00:02:00          # quit if job hangs after two hours
#SBATCH --job-name=pipeline_run

# load singularity module
module load singularity-3.5.3

# turn subjects into array
arr=()
while IFS= read -r line; do
  arr+=("$line")
done < subjects.txt

# use singularity container for each available subject and run in parallel 
singularity exec --bind /home/data/NDClab/data/base-eeg/CMI/derivatives,/home/data/NDClab/data/base-eeg/CMI/rawdata/${arr[${SLURM_ARRAY_TASK_ID}]} container/run-container.simg python3 run.py

# delete subjects.txt 
rm -f subjects.txt
